{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djfelrl11/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "url = \"./edm-cifar10-32x32-uncond-vp.pkl\"\n",
    "\n",
    "with open(url, \"rb\") as f:\n",
    "    net = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7eff3319a040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.keys()\n",
    "net['ema'].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.enc.32x32_conv.weight\n",
      "tensor([[[[ 0.0383,  0.0600,  0.0231,  ...,  0.0898, -0.0308, -0.0521],\n",
      "          [ 0.1773, -0.0993, -0.0464,  ...,  0.0443, -0.0330,  0.1241],\n",
      "          [ 0.0629, -0.1381, -0.2838,  ..., -0.2538,  0.0723,  0.0818]],\n",
      "\n",
      "         [[-0.0375,  0.2537, -0.2562,  ...,  0.1141, -0.0461, -0.4239],\n",
      "          [ 0.4320, -0.2542, -0.1735,  ..., -0.0800,  0.0096, -0.2441],\n",
      "          [ 0.3605, -0.2470, -0.1909,  ..., -0.6011, -0.1073, -0.0910]],\n",
      "\n",
      "         [[-0.3252,  0.1951, -0.2357,  ..., -0.1250,  0.0429, -0.1421],\n",
      "          [-0.1491, -0.1782,  0.0257,  ..., -0.0295,  0.0707, -0.1429],\n",
      "          [-0.0355, -0.3586,  0.1748,  ..., -0.3312,  0.0038, -0.0127]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4675, -0.0102,  0.3828,  ...,  0.2982, -0.1866, -0.2073],\n",
      "          [ 0.7167,  0.1763, -0.0081,  ..., -0.0347, -0.1040,  0.0323],\n",
      "          [ 0.3012,  0.3947, -0.2435,  ..., -0.5745,  0.0694, -0.0913]],\n",
      "\n",
      "         [[ 0.7642, -0.2769, -0.2717,  ...,  0.3505, -0.2819,  0.6648],\n",
      "          [ 0.0681, -1.5889,  1.1055,  ...,  0.8405, -0.3121,  0.3168],\n",
      "          [ 0.3778,  0.5803, -0.9409,  ...,  0.7297,  0.0041, -0.8270]],\n",
      "\n",
      "         [[-0.6730,  0.3983, -0.5994,  ..., -0.0832, -0.0621, -0.0726],\n",
      "          [-0.1456,  0.2569, -0.1369,  ..., -0.1202,  0.0378, -0.1707],\n",
      "          [-0.2134,  0.1946,  0.4605,  ..., -0.5917, -0.1258, -0.1071]]],\n",
      "\n",
      "\n",
      "        [[[-0.0933, -0.0624,  0.1823,  ..., -0.0312, -0.0413,  0.1857],\n",
      "          [-0.0344,  0.1311,  0.0508,  ...,  0.0130,  0.0180,  0.2012],\n",
      "          [-0.1983,  0.3054, -0.1660,  ..., -0.2071,  0.2316,  0.0170]],\n",
      "\n",
      "         [[-0.3309, -0.0267,  0.3787,  ...,  0.0201, -0.2169,  0.5911],\n",
      "          [ 0.1392,  0.5453,  0.3676,  ..., -0.2271, -0.0479,  0.4204],\n",
      "          [-0.2896,  0.7186,  0.5098,  ..., -0.6934,  0.1268,  0.2098]],\n",
      "\n",
      "         [[-0.3169, -0.1146, -0.0618,  ..., -0.1790, -0.0586,  0.1577],\n",
      "          [-0.1749, -0.0130,  0.0112,  ..., -0.0347,  0.0564, -0.0128],\n",
      "          [-0.1827,  0.0049,  0.1636,  ..., -0.2367,  0.0499,  0.0337]]]])\n",
      "torch.Size([3, 3, 3, 128])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, params in net['ema'].named_parameters():\n",
    "    # print(name)\n",
    "    if \"32x32_conv\" in name:\n",
    "        print(name)\n",
    "        print(params.data.permute(2, 3, 1, 0))\n",
    "        print(params.data.permute(2, 3, 1, 0).shape)\n",
    "        print()\n",
    "        break\n",
    "    # print(params)\n",
    "    # if input() == \".\":\n",
    "    #     break      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../diffusion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "from omegaconf import OmegaConf\n",
    "from framework.unifying_framework import UnifyingFramework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'framework': {'diffusion': {'type': 'edm', 'beta': [0.0001, 0.02], 'n_timestep': 18, 'loss': 'l2', 'noise_schedule': 'linear', 'learn_sigma': False, 'sigma_min': 0.002, 'sigma_max': 80, 'rho': 7, 'deterministic_sampling': True, 'S_churn': 30, 'S_min': 0.01, 'S_max': 1, 'S_noise': 1.007, 'train': {'learning_rate': 0.001, 'warmup': 19532, 'batch_size': 512, 'total_step': 400000, 'sampling_step': 10000, 'saving_step': 100000, 'optimizer': {'type': 'Adam'}}}, 'train_idx': 2}, 'model': {'autoencoder': {'image_channels': 3, 'n_channels': 128, 'ch_mults': [1, 2], 'is_atten': [False, False], 'n_blocks': 2, 'dropout_rate': 0.0, 'n_heads': 1, 'n_groups': 32, 'embed_dim': 4}, 'discriminator': {'disc_start': 50001, 'kl_weight': 1e-06, 'disc_weight': 0.5}, 'diffusion': {'type': 'unetpp', 'image_channels': 3, 'n_channels': 128, 'label_dim': 0, 'augment_dim': 9, 'ch_mults': [2, 2, 2], 'is_atten': [False, True, False], 'n_blocks': 4, 'n_heads': 1, 'n_groups': 32, 'dropout_rate': 0.13, 'label_dropout_rate': 0.0, 'embedding_type': 'positional', 'encoder_type': 'standard', 'decoder_type': 'standard', 'resample_filter': [1, 1], 'learn_sigma': False}}, 'ema': {'beta': 0.5, 'update_every': 1, 'update_after_step': 0, 'ema_rampup_ratio': 0.05, 'ema_halflife_number': 500000}, 'dataset': {'name': 'cifar10', 'data_size': [32, 32, 3]}, 'exp': {'exp_dir': 'experiments', 'current_exp_dir': '${exp.exp_dir}/${exp_name}', 'sampling_dir': '${exp.current_exp_dir}/sampling', 'in_process_dir': '${exp.current_exp_dir}/in_process', 'checkpoint_dir': '${exp.current_exp_dir}/checkpoints', 'best_dir': '${exp.checkpoint_dir}/best', 'autoencoder_prefix': 'ae_', 'discriminator_prefix': 'discriminator_', 'diffusion_prefix': 'diffusion_'}, 'type': '${framework.diffusion.type}', 'exp_name': 'edm_augment_pipe', 'do_training': True, 'do_sampling': True, 'num_sampling': 50000, 'sampling_batch': 256, 'rand_seed': 42, 'fid_during_training': True, 'n_jitted_steps': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djfelrl11/.local/lib/python3.8/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"../diffusion/configs/\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 0 loaded\n"
     ]
    }
   ],
   "source": [
    "config=cfg\n",
    "rng = jax.random.PRNGKey(config.rand_seed)\n",
    "model_type = config.type\n",
    "diffusion_framework = UnifyingFramework(model_type, config, rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['dec_16x16_block0', 'dec_16x16_block1', 'dec_16x16_block2', 'dec_16x16_block3', 'dec_16x16_block4', 'dec_16x16_up', 'dec_32x32_aux_conv', 'dec_32x32_aux_norm', 'dec_32x32_block0', 'dec_32x32_block1', 'dec_32x32_block2', 'dec_32x32_block3', 'dec_32x32_block4', 'dec_32x32_up', 'dec_8x8_block0', 'dec_8x8_block1', 'dec_8x8_block2', 'dec_8x8_block3', 'dec_8x8_block4', 'dec_8x8_in0', 'dec_8x8_in1', 'enc_16x16_block0', 'enc_16x16_block1', 'enc_16x16_block2', 'enc_16x16_block3', 'enc_16x16_down', 'enc_32x32_block0', 'enc_32x32_block1', 'enc_32x32_block2', 'enc_32x32_block3', 'enc_32x32_conv', 'enc_8x8_block0', 'enc_8x8_block1', 'enc_8x8_block2', 'enc_8x8_block3', 'enc_8x8_down', 'map_augment', 'map_layer0', 'map_layer1'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion_framework.framework.model_state.params['UNetpp_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, params in net['ema'].named_parameters():\n",
    "    break\n",
    "    print(name)\n",
    "    split_name = name.split(\".\")\n",
    "    print(split_name)\n",
    "    \n",
    "    if \"enc\" in split_name:\n",
    "        print() \n",
    "\n",
    "    if input() == \".\":\n",
    "        break\n",
    "    # print(params)\n",
    "    # if input() == \".\":\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "def pytorch_keys_to_flax_keys(pytorch_state_dict):\n",
    "  \"\"\"\n",
    "    Input: pytorch state_dict\n",
    "      The keys of pytorch state_dict consists of \n",
    "        `first_stage_model.encoder.conv_in.weight`, for example.\n",
    "    \n",
    "    Output: flax frozen_dict\n",
    "      The keys are converted to\n",
    "        `['first_stage_model']['encoder']['conv_in']['kernel'] (all `weight` is converted to `kernel`.)\n",
    "      Values will be the same, except for some transposes.\n",
    "\n",
    "    Weight of convolutional layers\n",
    "      pytorch: [out_channel, in_channel, kernel_h, kernel_w]\n",
    "      flax: [kernel_h, kernel_w, in_channel, out_channel]\n",
    "  \"\"\"\n",
    "  flax_tree = {}\n",
    "  for tup in pytorch_state_dict:\n",
    "    flax_key_flatten = tuple(tup.split(\".\"))\n",
    "    flax_value = jnp.array(pytorch_state_dict[tup])\n",
    "    flax_tree[flax_key_flatten] = flax_value\n",
    "  \n",
    "  flax_tree_unflattened = unflatten_dict(flax_tree)\n",
    "  return flax_tree_unflattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flax_dict = pytorch_keys_to_flax_keys(net['ema'].state_dict())\n",
    "flax_dict_flattened = flatten_dict(flax_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec.16x16_block0.affine.bias transferred.\n",
      "dec.16x16_block0.affine.weight transferred.\n",
      "dec.16x16_block0.conv0.bias transferred.\n",
      "dec.16x16_block0.conv0.weight transferred.\n",
      "dec.16x16_block0.conv1.bias transferred.\n",
      "dec.16x16_block0.conv1.weight transferred.\n",
      "dec.16x16_block0.norm0.bias transferred.\n",
      "dec.16x16_block0.norm0.weight transferred.\n",
      "dec.16x16_block0.norm1.bias transferred.\n",
      "dec.16x16_block0.norm1.weight transferred.\n",
      "dec.16x16_block0.skip.bias transferred.\n",
      "dec.16x16_block0.skip.weight transferred.\n",
      "dec.16x16_block1.affine.bias transferred.\n",
      "dec.16x16_block1.affine.weight transferred.\n",
      "dec.16x16_block1.conv0.bias transferred.\n",
      "dec.16x16_block1.conv0.weight transferred.\n",
      "dec.16x16_block1.conv1.bias transferred.\n",
      "dec.16x16_block1.conv1.weight transferred.\n",
      "dec.16x16_block1.norm0.bias transferred.\n",
      "dec.16x16_block1.norm0.weight transferred.\n",
      "dec.16x16_block1.norm1.bias transferred.\n",
      "dec.16x16_block1.norm1.weight transferred.\n",
      "dec.16x16_block1.skip.bias transferred.\n",
      "dec.16x16_block1.skip.weight transferred.\n",
      "dec.16x16_block2.affine.bias transferred.\n",
      "dec.16x16_block2.affine.weight transferred.\n",
      "dec.16x16_block2.conv0.bias transferred.\n",
      "dec.16x16_block2.conv0.weight transferred.\n",
      "dec.16x16_block2.conv1.bias transferred.\n",
      "dec.16x16_block2.conv1.weight transferred.\n",
      "dec.16x16_block2.norm0.bias transferred.\n",
      "dec.16x16_block2.norm0.weight transferred.\n",
      "dec.16x16_block2.norm1.bias transferred.\n",
      "dec.16x16_block2.norm1.weight transferred.\n",
      "dec.16x16_block2.skip.bias transferred.\n",
      "dec.16x16_block2.skip.weight transferred.\n",
      "dec.16x16_block3.affine.bias transferred.\n",
      "dec.16x16_block3.affine.weight transferred.\n",
      "dec.16x16_block3.conv0.bias transferred.\n",
      "dec.16x16_block3.conv0.weight transferred.\n",
      "dec.16x16_block3.conv1.bias transferred.\n",
      "dec.16x16_block3.conv1.weight transferred.\n",
      "dec.16x16_block3.norm0.bias transferred.\n",
      "dec.16x16_block3.norm0.weight transferred.\n",
      "dec.16x16_block3.norm1.bias transferred.\n",
      "dec.16x16_block3.norm1.weight transferred.\n",
      "dec.16x16_block3.skip.bias transferred.\n",
      "dec.16x16_block3.skip.weight transferred.\n",
      "dec.16x16_block4.affine.bias transferred.\n",
      "dec.16x16_block4.affine.weight transferred.\n",
      "dec.16x16_block4.norm2.bias transferred.\n",
      "dec.16x16_block4.norm2.weight transferred.\n",
      "dec.16x16_block4.proj.bias transferred.\n",
      "dec.16x16_block4.proj.weight transferred.\n",
      "dec.16x16_block4.qkv.bias transferred.\n",
      "dec.16x16_block4.qkv.weight transferred.\n",
      "dec.16x16_block4.conv0.bias transferred.\n",
      "dec.16x16_block4.conv0.weight transferred.\n",
      "dec.16x16_block4.conv1.bias transferred.\n",
      "dec.16x16_block4.conv1.weight transferred.\n",
      "dec.16x16_block4.norm0.bias transferred.\n",
      "dec.16x16_block4.norm0.weight transferred.\n",
      "dec.16x16_block4.norm1.bias transferred.\n",
      "dec.16x16_block4.norm1.weight transferred.\n",
      "dec.16x16_block4.skip.bias transferred.\n",
      "dec.16x16_block4.skip.weight transferred.\n",
      "dec.16x16_up.affine.bias transferred.\n",
      "dec.16x16_up.affine.weight transferred.\n",
      "dec.16x16_up.conv0.bias transferred.\n",
      "dec.16x16_up.conv0.weight transferred.\n",
      "dec.16x16_up.conv1.bias transferred.\n",
      "dec.16x16_up.conv1.weight transferred.\n",
      "dec.16x16_up.norm0.bias transferred.\n",
      "dec.16x16_up.norm0.weight transferred.\n",
      "dec.16x16_up.norm1.bias transferred.\n",
      "dec.16x16_up.norm1.weight transferred.\n",
      "dec.16x16_up.skip.bias transferred.\n",
      "dec.16x16_up.skip.weight transferred.\n",
      "dec.32x32_aux_conv.bias transferred.\n",
      "dec.32x32_aux_conv.weight transferred.\n",
      "dec.32x32_aux_norm.bias transferred.\n",
      "dec.32x32_aux_norm.weight transferred.\n",
      "dec.32x32_block0.affine.bias transferred.\n",
      "dec.32x32_block0.affine.weight transferred.\n",
      "dec.32x32_block0.conv0.bias transferred.\n",
      "dec.32x32_block0.conv0.weight transferred.\n",
      "dec.32x32_block0.conv1.bias transferred.\n",
      "dec.32x32_block0.conv1.weight transferred.\n",
      "dec.32x32_block0.norm0.bias transferred.\n",
      "dec.32x32_block0.norm0.weight transferred.\n",
      "dec.32x32_block0.norm1.bias transferred.\n",
      "dec.32x32_block0.norm1.weight transferred.\n",
      "dec.32x32_block0.skip.bias transferred.\n",
      "dec.32x32_block0.skip.weight transferred.\n",
      "dec.32x32_block1.affine.bias transferred.\n",
      "dec.32x32_block1.affine.weight transferred.\n",
      "dec.32x32_block1.conv0.bias transferred.\n",
      "dec.32x32_block1.conv0.weight transferred.\n",
      "dec.32x32_block1.conv1.bias transferred.\n",
      "dec.32x32_block1.conv1.weight transferred.\n",
      "dec.32x32_block1.norm0.bias transferred.\n",
      "dec.32x32_block1.norm0.weight transferred.\n",
      "dec.32x32_block1.norm1.bias transferred.\n",
      "dec.32x32_block1.norm1.weight transferred.\n",
      "dec.32x32_block1.skip.bias transferred.\n",
      "dec.32x32_block1.skip.weight transferred.\n",
      "dec.32x32_block2.affine.bias transferred.\n",
      "dec.32x32_block2.affine.weight transferred.\n",
      "dec.32x32_block2.conv0.bias transferred.\n",
      "dec.32x32_block2.conv0.weight transferred.\n",
      "dec.32x32_block2.conv1.bias transferred.\n",
      "dec.32x32_block2.conv1.weight transferred.\n",
      "dec.32x32_block2.norm0.bias transferred.\n",
      "dec.32x32_block2.norm0.weight transferred.\n",
      "dec.32x32_block2.norm1.bias transferred.\n",
      "dec.32x32_block2.norm1.weight transferred.\n",
      "dec.32x32_block2.skip.bias transferred.\n",
      "dec.32x32_block2.skip.weight transferred.\n",
      "dec.32x32_block3.affine.bias transferred.\n",
      "dec.32x32_block3.affine.weight transferred.\n",
      "dec.32x32_block3.conv0.bias transferred.\n",
      "dec.32x32_block3.conv0.weight transferred.\n",
      "dec.32x32_block3.conv1.bias transferred.\n",
      "dec.32x32_block3.conv1.weight transferred.\n",
      "dec.32x32_block3.norm0.bias transferred.\n",
      "dec.32x32_block3.norm0.weight transferred.\n",
      "dec.32x32_block3.norm1.bias transferred.\n",
      "dec.32x32_block3.norm1.weight transferred.\n",
      "dec.32x32_block3.skip.bias transferred.\n",
      "dec.32x32_block3.skip.weight transferred.\n",
      "dec.32x32_block4.affine.bias transferred.\n",
      "dec.32x32_block4.affine.weight transferred.\n",
      "dec.32x32_block4.conv0.bias transferred.\n",
      "dec.32x32_block4.conv0.weight transferred.\n",
      "dec.32x32_block4.conv1.bias transferred.\n",
      "dec.32x32_block4.conv1.weight transferred.\n",
      "dec.32x32_block4.norm0.bias transferred.\n",
      "dec.32x32_block4.norm0.weight transferred.\n",
      "dec.32x32_block4.norm1.bias transferred.\n",
      "dec.32x32_block4.norm1.weight transferred.\n",
      "dec.32x32_block4.skip.bias transferred.\n",
      "dec.32x32_block4.skip.weight transferred.\n",
      "dec.32x32_up.affine.bias transferred.\n",
      "dec.32x32_up.affine.weight transferred.\n",
      "dec.32x32_up.conv0.bias transferred.\n",
      "dec.32x32_up.conv0.weight transferred.\n",
      "dec.32x32_up.conv1.bias transferred.\n",
      "dec.32x32_up.conv1.weight transferred.\n",
      "dec.32x32_up.norm0.bias transferred.\n",
      "dec.32x32_up.norm0.weight transferred.\n",
      "dec.32x32_up.norm1.bias transferred.\n",
      "dec.32x32_up.norm1.weight transferred.\n",
      "dec.32x32_up.skip.bias transferred.\n",
      "dec.32x32_up.skip.weight transferred.\n",
      "dec.8x8_block0.affine.bias transferred.\n",
      "dec.8x8_block0.affine.weight transferred.\n",
      "dec.8x8_block0.conv0.bias transferred.\n",
      "dec.8x8_block0.conv0.weight transferred.\n",
      "dec.8x8_block0.conv1.bias transferred.\n",
      "dec.8x8_block0.conv1.weight transferred.\n",
      "dec.8x8_block0.norm0.bias transferred.\n",
      "dec.8x8_block0.norm0.weight transferred.\n",
      "dec.8x8_block0.norm1.bias transferred.\n",
      "dec.8x8_block0.norm1.weight transferred.\n",
      "dec.8x8_block0.skip.bias transferred.\n",
      "dec.8x8_block0.skip.weight transferred.\n",
      "dec.8x8_block1.affine.bias transferred.\n",
      "dec.8x8_block1.affine.weight transferred.\n",
      "dec.8x8_block1.conv0.bias transferred.\n",
      "dec.8x8_block1.conv0.weight transferred.\n",
      "dec.8x8_block1.conv1.bias transferred.\n",
      "dec.8x8_block1.conv1.weight transferred.\n",
      "dec.8x8_block1.norm0.bias transferred.\n",
      "dec.8x8_block1.norm0.weight transferred.\n",
      "dec.8x8_block1.norm1.bias transferred.\n",
      "dec.8x8_block1.norm1.weight transferred.\n",
      "dec.8x8_block1.skip.bias transferred.\n",
      "dec.8x8_block1.skip.weight transferred.\n",
      "dec.8x8_block2.affine.bias transferred.\n",
      "dec.8x8_block2.affine.weight transferred.\n",
      "dec.8x8_block2.conv0.bias transferred.\n",
      "dec.8x8_block2.conv0.weight transferred.\n",
      "dec.8x8_block2.conv1.bias transferred.\n",
      "dec.8x8_block2.conv1.weight transferred.\n",
      "dec.8x8_block2.norm0.bias transferred.\n",
      "dec.8x8_block2.norm0.weight transferred.\n",
      "dec.8x8_block2.norm1.bias transferred.\n",
      "dec.8x8_block2.norm1.weight transferred.\n",
      "dec.8x8_block2.skip.bias transferred.\n",
      "dec.8x8_block2.skip.weight transferred.\n",
      "dec.8x8_block3.affine.bias transferred.\n",
      "dec.8x8_block3.affine.weight transferred.\n",
      "dec.8x8_block3.conv0.bias transferred.\n",
      "dec.8x8_block3.conv0.weight transferred.\n",
      "dec.8x8_block3.conv1.bias transferred.\n",
      "dec.8x8_block3.conv1.weight transferred.\n",
      "dec.8x8_block3.norm0.bias transferred.\n",
      "dec.8x8_block3.norm0.weight transferred.\n",
      "dec.8x8_block3.norm1.bias transferred.\n",
      "dec.8x8_block3.norm1.weight transferred.\n",
      "dec.8x8_block3.skip.bias transferred.\n",
      "dec.8x8_block3.skip.weight transferred.\n",
      "dec.8x8_block4.affine.bias transferred.\n",
      "dec.8x8_block4.affine.weight transferred.\n",
      "dec.8x8_block4.conv0.bias transferred.\n",
      "dec.8x8_block4.conv0.weight transferred.\n",
      "dec.8x8_block4.conv1.bias transferred.\n",
      "dec.8x8_block4.conv1.weight transferred.\n",
      "dec.8x8_block4.norm0.bias transferred.\n",
      "dec.8x8_block4.norm0.weight transferred.\n",
      "dec.8x8_block4.norm1.bias transferred.\n",
      "dec.8x8_block4.norm1.weight transferred.\n",
      "dec.8x8_block4.skip.bias transferred.\n",
      "dec.8x8_block4.skip.weight transferred.\n",
      "dec.8x8_in0.affine.bias transferred.\n",
      "dec.8x8_in0.affine.weight transferred.\n",
      "dec.8x8_in0.norm2.bias transferred.\n",
      "dec.8x8_in0.norm2.weight transferred.\n",
      "dec.8x8_in0.proj.bias transferred.\n",
      "dec.8x8_in0.proj.weight transferred.\n",
      "dec.8x8_in0.qkv.bias transferred.\n",
      "dec.8x8_in0.qkv.weight transferred.\n",
      "dec.8x8_in0.conv0.bias transferred.\n",
      "dec.8x8_in0.conv0.weight transferred.\n",
      "dec.8x8_in0.conv1.bias transferred.\n",
      "dec.8x8_in0.conv1.weight transferred.\n",
      "dec.8x8_in0.norm0.bias transferred.\n",
      "dec.8x8_in0.norm0.weight transferred.\n",
      "dec.8x8_in0.norm1.bias transferred.\n",
      "dec.8x8_in0.norm1.weight transferred.\n",
      "dec.8x8_in1.affine.bias transferred.\n",
      "dec.8x8_in1.affine.weight transferred.\n",
      "dec.8x8_in1.conv0.bias transferred.\n",
      "dec.8x8_in1.conv0.weight transferred.\n",
      "dec.8x8_in1.conv1.bias transferred.\n",
      "dec.8x8_in1.conv1.weight transferred.\n",
      "dec.8x8_in1.norm0.bias transferred.\n",
      "dec.8x8_in1.norm0.weight transferred.\n",
      "dec.8x8_in1.norm1.bias transferred.\n",
      "dec.8x8_in1.norm1.weight transferred.\n",
      "enc.16x16_block0.affine.bias transferred.\n",
      "enc.16x16_block0.affine.weight transferred.\n",
      "enc.16x16_block0.norm2.bias transferred.\n",
      "enc.16x16_block0.norm2.weight transferred.\n",
      "enc.16x16_block0.proj.bias transferred.\n",
      "enc.16x16_block0.proj.weight transferred.\n",
      "enc.16x16_block0.qkv.bias transferred.\n",
      "enc.16x16_block0.qkv.weight transferred.\n",
      "enc.16x16_block0.conv0.bias transferred.\n",
      "enc.16x16_block0.conv0.weight transferred.\n",
      "enc.16x16_block0.conv1.bias transferred.\n",
      "enc.16x16_block0.conv1.weight transferred.\n",
      "enc.16x16_block0.norm0.bias transferred.\n",
      "enc.16x16_block0.norm0.weight transferred.\n",
      "enc.16x16_block0.norm1.bias transferred.\n",
      "enc.16x16_block0.norm1.weight transferred.\n",
      "enc.16x16_block1.affine.bias transferred.\n",
      "enc.16x16_block1.affine.weight transferred.\n",
      "enc.16x16_block1.norm2.bias transferred.\n",
      "enc.16x16_block1.norm2.weight transferred.\n",
      "enc.16x16_block1.proj.bias transferred.\n",
      "enc.16x16_block1.proj.weight transferred.\n",
      "enc.16x16_block1.qkv.bias transferred.\n",
      "enc.16x16_block1.qkv.weight transferred.\n",
      "enc.16x16_block1.conv0.bias transferred.\n",
      "enc.16x16_block1.conv0.weight transferred.\n",
      "enc.16x16_block1.conv1.bias transferred.\n",
      "enc.16x16_block1.conv1.weight transferred.\n",
      "enc.16x16_block1.norm0.bias transferred.\n",
      "enc.16x16_block1.norm0.weight transferred.\n",
      "enc.16x16_block1.norm1.bias transferred.\n",
      "enc.16x16_block1.norm1.weight transferred.\n",
      "enc.16x16_block2.affine.bias transferred.\n",
      "enc.16x16_block2.affine.weight transferred.\n",
      "enc.16x16_block2.norm2.bias transferred.\n",
      "enc.16x16_block2.norm2.weight transferred.\n",
      "enc.16x16_block2.proj.bias transferred.\n",
      "enc.16x16_block2.proj.weight transferred.\n",
      "enc.16x16_block2.qkv.bias transferred.\n",
      "enc.16x16_block2.qkv.weight transferred.\n",
      "enc.16x16_block2.conv0.bias transferred.\n",
      "enc.16x16_block2.conv0.weight transferred.\n",
      "enc.16x16_block2.conv1.bias transferred.\n",
      "enc.16x16_block2.conv1.weight transferred.\n",
      "enc.16x16_block2.norm0.bias transferred.\n",
      "enc.16x16_block2.norm0.weight transferred.\n",
      "enc.16x16_block2.norm1.bias transferred.\n",
      "enc.16x16_block2.norm1.weight transferred.\n",
      "enc.16x16_block3.affine.bias transferred.\n",
      "enc.16x16_block3.affine.weight transferred.\n",
      "enc.16x16_block3.norm2.bias transferred.\n",
      "enc.16x16_block3.norm2.weight transferred.\n",
      "enc.16x16_block3.proj.bias transferred.\n",
      "enc.16x16_block3.proj.weight transferred.\n",
      "enc.16x16_block3.qkv.bias transferred.\n",
      "enc.16x16_block3.qkv.weight transferred.\n",
      "enc.16x16_block3.conv0.bias transferred.\n",
      "enc.16x16_block3.conv0.weight transferred.\n",
      "enc.16x16_block3.conv1.bias transferred.\n",
      "enc.16x16_block3.conv1.weight transferred.\n",
      "enc.16x16_block3.norm0.bias transferred.\n",
      "enc.16x16_block3.norm0.weight transferred.\n",
      "enc.16x16_block3.norm1.bias transferred.\n",
      "enc.16x16_block3.norm1.weight transferred.\n",
      "enc.16x16_down.affine.bias transferred.\n",
      "enc.16x16_down.affine.weight transferred.\n",
      "enc.16x16_down.conv0.bias transferred.\n",
      "enc.16x16_down.conv0.weight transferred.\n",
      "enc.16x16_down.conv1.bias transferred.\n",
      "enc.16x16_down.conv1.weight transferred.\n",
      "enc.16x16_down.norm0.bias transferred.\n",
      "enc.16x16_down.norm0.weight transferred.\n",
      "enc.16x16_down.norm1.bias transferred.\n",
      "enc.16x16_down.norm1.weight transferred.\n",
      "enc.16x16_down.skip.bias transferred.\n",
      "enc.16x16_down.skip.weight transferred.\n",
      "enc.32x32_block0.affine.bias transferred.\n",
      "enc.32x32_block0.affine.weight transferred.\n",
      "enc.32x32_block0.conv0.bias transferred.\n",
      "enc.32x32_block0.conv0.weight transferred.\n",
      "enc.32x32_block0.conv1.bias transferred.\n",
      "enc.32x32_block0.conv1.weight transferred.\n",
      "enc.32x32_block0.norm0.bias transferred.\n",
      "enc.32x32_block0.norm0.weight transferred.\n",
      "enc.32x32_block0.norm1.bias transferred.\n",
      "enc.32x32_block0.norm1.weight transferred.\n",
      "enc.32x32_block0.skip.bias transferred.\n",
      "enc.32x32_block0.skip.weight transferred.\n",
      "enc.32x32_block1.affine.bias transferred.\n",
      "enc.32x32_block1.affine.weight transferred.\n",
      "enc.32x32_block1.conv0.bias transferred.\n",
      "enc.32x32_block1.conv0.weight transferred.\n",
      "enc.32x32_block1.conv1.bias transferred.\n",
      "enc.32x32_block1.conv1.weight transferred.\n",
      "enc.32x32_block1.norm0.bias transferred.\n",
      "enc.32x32_block1.norm0.weight transferred.\n",
      "enc.32x32_block1.norm1.bias transferred.\n",
      "enc.32x32_block1.norm1.weight transferred.\n",
      "enc.32x32_block2.affine.bias transferred.\n",
      "enc.32x32_block2.affine.weight transferred.\n",
      "enc.32x32_block2.conv0.bias transferred.\n",
      "enc.32x32_block2.conv0.weight transferred.\n",
      "enc.32x32_block2.conv1.bias transferred.\n",
      "enc.32x32_block2.conv1.weight transferred.\n",
      "enc.32x32_block2.norm0.bias transferred.\n",
      "enc.32x32_block2.norm0.weight transferred.\n",
      "enc.32x32_block2.norm1.bias transferred.\n",
      "enc.32x32_block2.norm1.weight transferred.\n",
      "enc.32x32_block3.affine.bias transferred.\n",
      "enc.32x32_block3.affine.weight transferred.\n",
      "enc.32x32_block3.conv0.bias transferred.\n",
      "enc.32x32_block3.conv0.weight transferred.\n",
      "enc.32x32_block3.conv1.bias transferred.\n",
      "enc.32x32_block3.conv1.weight transferred.\n",
      "enc.32x32_block3.norm0.bias transferred.\n",
      "enc.32x32_block3.norm0.weight transferred.\n",
      "enc.32x32_block3.norm1.bias transferred.\n",
      "enc.32x32_block3.norm1.weight transferred.\n",
      "enc.32x32_conv.bias transferred.\n",
      "enc.32x32_conv.weight transferred.\n",
      "enc.8x8_block0.affine.bias transferred.\n",
      "enc.8x8_block0.affine.weight transferred.\n",
      "enc.8x8_block0.conv0.bias transferred.\n",
      "enc.8x8_block0.conv0.weight transferred.\n",
      "enc.8x8_block0.conv1.bias transferred.\n",
      "enc.8x8_block0.conv1.weight transferred.\n",
      "enc.8x8_block0.norm0.bias transferred.\n",
      "enc.8x8_block0.norm0.weight transferred.\n",
      "enc.8x8_block0.norm1.bias transferred.\n",
      "enc.8x8_block0.norm1.weight transferred.\n",
      "enc.8x8_block1.affine.bias transferred.\n",
      "enc.8x8_block1.affine.weight transferred.\n",
      "enc.8x8_block1.conv0.bias transferred.\n",
      "enc.8x8_block1.conv0.weight transferred.\n",
      "enc.8x8_block1.conv1.bias transferred.\n",
      "enc.8x8_block1.conv1.weight transferred.\n",
      "enc.8x8_block1.norm0.bias transferred.\n",
      "enc.8x8_block1.norm0.weight transferred.\n",
      "enc.8x8_block1.norm1.bias transferred.\n",
      "enc.8x8_block1.norm1.weight transferred.\n",
      "enc.8x8_block2.affine.bias transferred.\n",
      "enc.8x8_block2.affine.weight transferred.\n",
      "enc.8x8_block2.conv0.bias transferred.\n",
      "enc.8x8_block2.conv0.weight transferred.\n",
      "enc.8x8_block2.conv1.bias transferred.\n",
      "enc.8x8_block2.conv1.weight transferred.\n",
      "enc.8x8_block2.norm0.bias transferred.\n",
      "enc.8x8_block2.norm0.weight transferred.\n",
      "enc.8x8_block2.norm1.bias transferred.\n",
      "enc.8x8_block2.norm1.weight transferred.\n",
      "enc.8x8_block3.affine.bias transferred.\n",
      "enc.8x8_block3.affine.weight transferred.\n",
      "enc.8x8_block3.conv0.bias transferred.\n",
      "enc.8x8_block3.conv0.weight transferred.\n",
      "enc.8x8_block3.conv1.bias transferred.\n",
      "enc.8x8_block3.conv1.weight transferred.\n",
      "enc.8x8_block3.norm0.bias transferred.\n",
      "enc.8x8_block3.norm0.weight transferred.\n",
      "enc.8x8_block3.norm1.bias transferred.\n",
      "enc.8x8_block3.norm1.weight transferred.\n",
      "enc.8x8_down.affine.bias transferred.\n",
      "enc.8x8_down.affine.weight transferred.\n",
      "enc.8x8_down.conv0.bias transferred.\n",
      "enc.8x8_down.conv0.weight transferred.\n",
      "enc.8x8_down.conv1.bias transferred.\n",
      "enc.8x8_down.conv1.weight transferred.\n",
      "enc.8x8_down.norm0.bias transferred.\n",
      "enc.8x8_down.norm0.weight transferred.\n",
      "enc.8x8_down.norm1.bias transferred.\n",
      "enc.8x8_down.norm1.weight transferred.\n",
      "enc.8x8_down.skip.bias transferred.\n",
      "enc.8x8_down.skip.weight transferred.\n",
      "map_augment.weight transferred.\n",
      "map_layer0.bias transferred.\n",
      "map_layer0.weight transferred.\n",
      "map_layer1.bias transferred.\n",
      "map_layer1.weight transferred.\n",
      "[('enc', '16x16_down', 'conv0', 'resample_filter'), ('enc', '16x16_down', 'skip', 'resample_filter'), ('enc', '8x8_down', 'conv0', 'resample_filter'), ('enc', '8x8_down', 'skip', 'resample_filter'), ('dec', '16x16_up', 'conv0', 'resample_filter'), ('dec', '16x16_up', 'skip', 'resample_filter'), ('dec', '32x32_up', 'conv0', 'resample_filter'), ('dec', '32x32_up', 'skip', 'resample_filter')] of weights remained.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from flax.core import unfreeze, freeze\n",
    "from flax.jax_utils import unreplicate\n",
    "\n",
    "params= unreplicate(diffusion_framework.framework.model_state)\n",
    "params_unfreeze = unfreeze(flatten_dict(params.params))\n",
    "non_changing_els = [\"map_layer0\", \"map_layer1\", \"map_augment\"]\n",
    "total_weight = list(flax_dict_flattened.keys())\n",
    "\n",
    "for k, v in params_unfreeze.items():\n",
    "  # print(k)\n",
    "  # input()\n",
    "  k_ = tuple()\n",
    "  \n",
    "  for el in k:\n",
    "    if el == \"UNetpp_0\":\n",
    "      continue\n",
    "    elif el == \"atten\":\n",
    "      continue\n",
    "    elif 'enc' in el or 'dec' in el:\n",
    "      tmp_el = el.split(\"_\")\n",
    "      el = (tmp_el[0], \"_\".join(tmp_el[1:]),)\n",
    "    elif el not in non_changing_els:\n",
    "      el = tuple(el.split(\"_\"))\n",
    "    else:\n",
    "      el = (el,)\n",
    "\n",
    "    if el == (\"kernel\",) or el == (\"scale\",):\n",
    "      k_ += (\"weight\",)\n",
    "    else:\n",
    "      k_ += el\n",
    "\n",
    "  if k_ in flax_dict_flattened.keys():\n",
    "    pytorch_dict_value = flax_dict_flattened[k_]\n",
    "    if len(v.shape) == 4:\n",
    "      pytorch_dict_value_transposed = jnp.transpose(pytorch_dict_value, (2, 3, 1, 0))\n",
    "      assert v.shape == pytorch_dict_value_transposed.shape\n",
    "      params_unfreeze[k] = pytorch_dict_value_transposed\n",
    "      # print(\"transferred\", flush=True)\n",
    "    elif len(v.shape) == 1:\n",
    "      assert v.shape == pytorch_dict_value.shape\n",
    "      params_unfreeze[k] = pytorch_dict_value\n",
    "      # print(\"transferred\", flush=True)\n",
    "    elif len(v.shape) == 2:\n",
    "      pytorch_dict_value_transposed = jnp.transpose(pytorch_dict_value, (1, 0))\n",
    "      assert v.shape == pytorch_dict_value_transposed.shape\n",
    "      params_unfreeze[k] = pytorch_dict_value_transposed\n",
    "    else:\n",
    "      print(v.shape)\n",
    "      raise ValueError(\"Exist?\")\n",
    "    print(f\"{'.'.join(k_)} transferred.\")\n",
    "    total_weight.remove(k_)\n",
    "  else:\n",
    "    print()\n",
    "    raise ValueError(f\"{'.'.join(k_)} is not existed in pytorch model\")\n",
    "\n",
    "print(f\"{total_weight} of weights remained.\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_unfreeze[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('enc', '16x16_down', 'conv0', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('enc', '16x16_down', 'skip', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('enc', '8x8_down', 'conv0', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('enc', '8x8_down', 'skip', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('dec', '16x16_up', 'conv0', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('dec', '16x16_up', 'skip', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('dec', '32x32_up', 'conv0', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n",
      "('dec', '32x32_up', 'skip', 'resample_filter')\n",
      "(1, 1, 2, 2)\n",
      "[[[[0.25 0.25]\n",
      "   [0.25 0.25]]]]\n"
     ]
    }
   ],
   "source": [
    "for key in total_weight:\n",
    "    pytorch_dict_value = flax_dict_flattened[key]\n",
    "    print(key)\n",
    "    print(pytorch_dict_value.shape)\n",
    "    print(pytorch_dict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_freeze = freeze(unflatten_dict(params_unfreeze))\n",
    "new_state = unreplicate(diffusion_framework.framework.model_state)\n",
    "new_state = new_state.replace(params_ema=params_freeze)\n",
    "new_state = new_state.replace(params=params_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoint_0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flax.training import checkpoints\n",
    "checkpoints.save_checkpoint(\".\", new_state, 0, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3, 3, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flax_dict_flattened[('enc', '32x32_conv', 'weight')].transpose(1, 0, 2, 3)\n",
    "flax_dict_flattened[('enc', '32x32_conv', 'weight')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[[ 0.03828168,  0.06004289,  0.02312655, ...,  0.08975515,\n",
       "          -0.03079157, -0.05214899],\n",
       "         [ 0.17726456, -0.099301  , -0.04637972, ...,  0.04428555,\n",
       "          -0.03303021,  0.12414239],\n",
       "         [ 0.06285588, -0.13811347, -0.28377378, ..., -0.25381586,\n",
       "           0.07231837,  0.081753  ]],\n",
       "\n",
       "        [[-0.03752021,  0.25367603, -0.25618958, ...,  0.11411095,\n",
       "          -0.0461234 , -0.42390025],\n",
       "         [ 0.43203157, -0.25416735, -0.17350987, ..., -0.07997738,\n",
       "           0.0096081 , -0.2440669 ],\n",
       "         [ 0.36049223, -0.24701966, -0.19088207, ..., -0.6011314 ,\n",
       "          -0.10726789, -0.091041  ]],\n",
       "\n",
       "        [[-0.3252467 ,  0.19510251, -0.23572196, ..., -0.12497745,\n",
       "           0.04286664, -0.14211504],\n",
       "         [-0.1490635 , -0.17821302,  0.02569932, ..., -0.02951242,\n",
       "           0.07066312, -0.14288044],\n",
       "         [-0.03545241, -0.358608  ,  0.17481224, ..., -0.33122206,\n",
       "           0.00377374, -0.01273975]]],\n",
       "\n",
       "\n",
       "       [[[ 0.46754178, -0.01022834,  0.3828126 , ...,  0.29824656,\n",
       "          -0.18656306, -0.20732984],\n",
       "         [ 0.7167497 ,  0.17631939, -0.00811464, ..., -0.03468664,\n",
       "          -0.10398552,  0.03226085],\n",
       "         [ 0.30121472,  0.39468318, -0.24346277, ..., -0.57449865,\n",
       "           0.06935251, -0.09129482]],\n",
       "\n",
       "        [[ 0.7641619 , -0.27692682, -0.27171135, ...,  0.35051066,\n",
       "          -0.2818525 ,  0.664767  ],\n",
       "         [ 0.06811061, -1.5889196 ,  1.1055019 , ...,  0.8404996 ,\n",
       "          -0.3121351 ,  0.3167869 ],\n",
       "         [ 0.37776768,  0.58027697, -0.9408528 , ...,  0.7297211 ,\n",
       "           0.00412691, -0.82703775]],\n",
       "\n",
       "        [[-0.6730458 ,  0.39833912, -0.5994138 , ..., -0.08322466,\n",
       "          -0.06208742, -0.07258613],\n",
       "         [-0.14563504,  0.25689733, -0.13691412, ..., -0.12018063,\n",
       "           0.03781677, -0.17067978],\n",
       "         [-0.21337852,  0.19457227,  0.4604584 , ..., -0.59172875,\n",
       "          -0.12575833, -0.10712604]]],\n",
       "\n",
       "\n",
       "       [[[-0.09326348, -0.06241643,  0.1822919 , ..., -0.03115823,\n",
       "          -0.04128322,  0.18565871],\n",
       "         [-0.03442448,  0.13113412,  0.05083999, ...,  0.01300522,\n",
       "           0.01798029,  0.2012307 ],\n",
       "         [-0.19828525,  0.30538505, -0.16595525, ..., -0.20712969,\n",
       "           0.23155963,  0.01699064]],\n",
       "\n",
       "        [[-0.33090782, -0.02672457,  0.37867504, ...,  0.02012791,\n",
       "          -0.21685562,  0.5911321 ],\n",
       "         [ 0.1391535 ,  0.5452868 ,  0.3675822 , ..., -0.22705159,\n",
       "          -0.04788187,  0.42036706],\n",
       "         [-0.28964913,  0.71864295,  0.50977814, ..., -0.693355  ,\n",
       "           0.12683584,  0.20981163]],\n",
       "\n",
       "        [[-0.31688783, -0.11458667, -0.06179002, ..., -0.17900105,\n",
       "          -0.05864458,  0.15771125],\n",
       "         [-0.17490567, -0.01297519,  0.01117762, ..., -0.03467638,\n",
       "           0.05642826, -0.01284914],\n",
       "         [-0.18273102,  0.00491214,  0.16360223, ..., -0.23669231,\n",
       "           0.04989   ,  0.03374388]]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_freeze['UNetpp_0']['enc_32x32_conv']['weight'].shape\n",
    "params_freeze['UNetpp_0']['enc_32x32_conv']['weight']\n",
    "# params_freeze['UNetpp_0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
