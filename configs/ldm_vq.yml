# Define experiment name and its path
exp:
  exp_name: "ldm_vq_cifar10_1211"
  exp_dir: "experiments"
  sampling_dir: "sampling"
  in_process_dir : "in_process"
  checkpoint_dir: "checkpoints"
  # checkpoint_prefix: [['ae_', "discriminator_"], 'diffusion_']
  autoencoder_prefix: 'ae_'
  discriminator_prefix: 'discriminator_'
  diffusion_prefix: 'diffusion_'

dataset: "cifar10"

# Define configuration of model
model:
  autoencoder:
    image_channels: 3
    n_channels: 128
    ch_mults: [1, 2] # For cifar 10
    is_atten: [False, False]
    n_blocks: 2
    dropout_rate: 0.0
    n_heads: 1
    n_groups: 32 
    embed_dim: 3
    n_embed: 8192
  
  discriminator:
    disc_start: 0
    codebook_weight: 1.0
    disc_weight: 0.75

  diffusion:
    image_channels: 3
    # n_channels: 128
    n_channels: 224
    ch_mults: [1, 2, 4] # For cifar 10
    is_atten: [False, True, False]
    n_blocks: 2
    dropout_rate: 0.0
    n_heads: 1
    n_groups: 32

# Define DDPM
framework:
  train_idx: 1 # 1 (AE) or 2 (Diffusion). This is only for ldm. Selected model will be trained. 
  fid_during_training: false # True when calculating fid during training, otherwise, false.
  autoencoder:
    mode: "VQ" # VQ or KL
    train:
      learning_rate: 4.5e-06
      gradient_clip: 1.0
      warmup: 5000
      batch_size: 128
      total_step: 500000
      sampling_step: 10000
      saving_step: 100000
      optimizer:
        type: "Adam"
        betas: [0.5, 0.9]
  diffusion:
    beta: [0.0001, 0.02]
    n_timestep: 1000
    loss: "l2"
    train:
      learning_rate: 2.0e-4
      gradient_clip: 1.0
      warmup: 5000
      batch_size: 128
      total_step: 500000
      sampling_step: 10000
      saving_step: 100000
      optimizer:
        type: "Adam"

# Define EMA configuration
ema:
  beta: 0.9999
  update_every: 10

sampling:
  batch_size: 256
  
rand_seed: 42
