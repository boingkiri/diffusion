type: "cm"
n_timestep: 18
loss: lpips # l2, l1, lpips
# noise_schedule: linear # cosine, linear
learn_sigma: False

# Consistency distillation or Consistency training
is_distillation: False
is_progressive_training: True
# distillation_path: experiments/edm_with_modified_attention_bad # Exp name or pretrained model file name
distillation_path: null # Exp name or pretrained model file name
target_model_ema_decay: 0
params_ema_for_training: [0.9, 2, 150] # mu_0, s_0, s_1

t_emb_output: True

# Sigma : Assume that the process is performing VP
sigma_min: 0.002
sigma_max: 80
rho: 7

beta: 9
traj_scale: 1e+5

deterministic_sampling: True
S_churn: 30
S_min: 0.01
S_max: 1
S_noise: 1.007

# augment_rate: 0.12

train:
  learning_rate: 4.0e-4
  # gradient_clip: 1.0
  # warmup: 5000
  # warmup: 19532 # int(10e6 / ${.batch_size})
  batch_size: 512
  total_step: 800000
  sampling_step: 10000
  saving_step: 100000
  optimizer:
    type: "radam"
