type: "cm"
n_timestep: 18
loss: huber # l2, l1, lpips
learn_sigma: False

# Consistency distillation or Consistency training
params_ema_for_training: [0.9, 10, 1280] # mu_0, s_0, s_1

# Sigma : Assume that the process is performing VP
sigma_min: 0.002
sigma_max: 80
rho: 7

# Option for sigma sampling for joint training
# EDM, iCT
sigma_sampling_joint: iCT


gradient_flow_from_head: True

# Connection loss (new loss)
alignment_loss: True
alignment_threshold: 50000
alignment_loss_scale: 1.0
alignment_loss_weight: "lognormal" # lognormal, uniform

train:
  learning_rate: 1.0e-4
  # total_batch_size: 1024
  # batch_size_per_rounds: 256 # The option for gradient accumulation, which is used for large batch size training.
  total_batch_size: 128
  batch_size_per_rounds: 128
  total_step: 400000
  sampling_step: 10000
  saving_step: 100000
  optimizer:
    type: "radam"
