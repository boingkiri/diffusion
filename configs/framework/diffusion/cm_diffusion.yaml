type: "cm"
n_timestep: 18
loss: lpips # l2, l1, lpips
# noise_schedule: linear # cosine, linear
learn_sigma: False

# Consistency distillation or Consistency training
is_distillation: False
distillation_path: experiments/edm_with_modified_attention_bad # Exp name or pretrained model file name
target_model_ema_decay: 0
params_ema_for_training: [0.9, 2, 150] # mu_0, s_0, s_1

# Sigma : Assume that the process is performing VP
sigma_min: 0.002
sigma_max: 80
rho: 7

deterministic_sampling: True
S_churn: 30
S_min: 0.01
S_max: 1
S_noise: 1.007

# Option for freezing CM during training
# CM_freeze: True
CM_freeze: False
lpips_loss_training: False # True

# Option for sigma sampling for head, torso
sigma_sampling_head: EDM # EDM, CM
sigma_sampling_torso: CM # EDM, CM


# augment_rate: 0.12
# loss weight for scoe_pde_head: dh_dx, dh_dt
score_pde_regularizer: False
dh_dx_inv_weight: 1.0
dh_dt_weight: 1.0

train:
  learning_rate: 4.0e-4
  # gradient_clip: 1.0
  # warmup: 5000
  # warmup: 19532 # int(10e6 / ${.batch_size})
  batch_size: 512
  total_step: 800000
  sampling_step: 10000
  saving_step: 100000
  optimizer:
    type: "radam"
